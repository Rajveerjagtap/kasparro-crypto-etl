# Kasparro - Crypto Data ETL & API Platform

[![CI Pipeline](https://github.com/Rajveerjagtap/kasparro-crypto-etl/actions/workflows/ci.yml/badge.svg)](https://github.com/Rajveerjagtap/kasparro-crypto-etl/actions/workflows/ci.yml)
[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A production-grade ETL and Backend system for ingesting cryptocurrency data from multiple sources (CoinPaprika, CoinGecko, CSV), normalizing it into a unified schema, and exposing it via a RESTful API.

## ğŸš€ Live Demo

**ğŸŒ Base URL:** [`https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io`](https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io)

### Quick Verification (Click to Test)

| Endpoint | URL | Description |
|----------|-----|-------------|
| ğŸ“– **API Docs** | [/docs](https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io/docs) | Interactive Swagger UI |
| â¤ï¸ **Health Check** | [/health](https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io/health) | System health status |
| ğŸ“Š **Statistics** | [/api/v1/stats](https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io/api/v1/stats) | Data statistics & ETL status |
| ğŸ“ˆ **Crypto Data** | [/api/v1/data](https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io/api/v1/data) | Retrieved cryptocurrency data |
| ğŸ”§ **ETL Jobs** | [/api/v1/etl/jobs](https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io/api/v1/etl/jobs) | ETL job history |
| ğŸ“‰ **Metrics** | [/api/v1/metrics](https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io/api/v1/metrics) | Prometheus metrics |

### Sample API Calls

```bash
# Health check
curl https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io/health

# Get crypto data (with BTC filter)
curl "https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io/api/v1/data?symbol=BTC"

# Trigger ETL job (sync mode)
curl -X POST "https://kasparro-backend.thankfulwave-f9a1a1b4.eastus2.azurecontainerapps.io/api/v1/etl/run/csv?sync=true"
```

---

## âœ¨ Features

### Data Ingestion
- **Multi-Source Support**: CoinGecko API, CoinPaprika API, and CSV files
- **Unified Normalization**: All sources normalized to a canonical `UnifiedCryptoData` schema
- **Symbol Normalization**: Fuzzy matching maps aliases (e.g., "bitcoin" â†’ "BTC")
- **Idempotent Upserts**: `ON CONFLICT DO UPDATE` ensures no duplicate records
- **Raw Data Auditability**: Original payloads preserved in `raw_data` table

### ETL Pipeline
- **Scheduled Execution**: APScheduler runs ETL jobs at configurable intervals
- **Incremental Loading**: Checkpoint-based processing for efficiency
- **Drift Detection**: Schema and data quality drift monitoring
- **Retry Logic**: Exponential backoff for API rate limits

### API & Observability
- **RESTful API**: FastAPI with automatic OpenAPI documentation
- **Health Checks**: Deep checks on database and ETL status
- **Prometheus Metrics**: Request counts, latency, ETL job statistics
- **Structured Logging**: JSON logs with request tracing (container-aware)

### Production Ready
- **Async-First**: Built with `asyncio`, `asyncpg`, and `httpx`
- **Validation**: Pydantic v2
- **Containerization**: Docker & Docker Compose
- **Database Migrations**: Alembic for schema versioning
- **Testing**: Pytest with 74 tests
- **CI/CD Pipeline**: GitHub Actions with lint, test, build, and deploy stages

---

## ğŸ›  Tech Stack

| Category | Technology |
|----------|------------|
| **Language** | Python 3.11+ |
| **Web Framework** | FastAPI 0.109 |
| **Database** | PostgreSQL 15 with asyncpg |
| **ORM** | SQLAlchemy 2.0 (async) |
| **Validation** | Pydantic v2 |
| **Scheduler** | APScheduler |
| **HTTP Client** | httpx (async) |
| **Containerization** | Docker & Docker Compose |
| **CI/CD** | GitHub Actions |
| **Cloud** | Azure Container Apps |
| **IaC** | Bicep |

---

## ğŸ“ Project Structure

```
kasparro-crypto-etl/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                    # FastAPI routes
â”‚   â”‚   â””â”€â”€ routes.py           # API endpoint definitions
â”‚   â”œâ”€â”€ core/                   # Core utilities
â”‚   â”‚   â”œâ”€â”€ config.py           # Pydantic settings
â”‚   â”‚   â”œâ”€â”€ exceptions.py       # Custom exceptions
â”‚   â”‚   â”œâ”€â”€ logging.py          # Container-aware logging
â”‚   â”‚   â””â”€â”€ middleware.py       # Request logging & metrics
â”‚   â”œâ”€â”€ db/                     # Database layer
â”‚   â”‚   â”œâ”€â”€ models.py           # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ session.py          # Async session management
â”‚   â”œâ”€â”€ ingestion/              # ETL pipeline
â”‚   â”‚   â”œâ”€â”€ base.py             # BaseExtractor ABC
â”‚   â”‚   â”œâ”€â”€ drift.py            # Drift detection
â”‚   â”‚   â”œâ”€â”€ normalization.py    # Symbol normalizer
â”‚   â”‚   â”œâ”€â”€ service.py          # ETL orchestration
â”‚   â”‚   â”œâ”€â”€ extractors/         # Source-specific extractors
â”‚   â”‚   â”‚   â”œâ”€â”€ coingecko.py
â”‚   â”‚   â”‚   â”œâ”€â”€ coinpaprika.py
â”‚   â”‚   â”‚   â””â”€â”€ csv_extractor.py
â”‚   â”‚   â””â”€â”€ transformers/
â”‚   â”‚       â””â”€â”€ schemas.py      # Intermediate validation schemas
â”‚   â”œâ”€â”€ schemas/                # API schemas
â”‚   â”‚   â””â”€â”€ crypto.py           # Request/response models
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â””â”€â”€ scheduler.py            # APScheduler ETL runner
â”œâ”€â”€ alembic/                    # Database migrations
â”‚   â””â”€â”€ versions/
â”‚       â””â”€â”€ 001_initial_schema.py
â”œâ”€â”€ azure/                      # Azure deployment
â”‚   â”œâ”€â”€ main.bicep              # Infrastructure as Code
â”‚   â””â”€â”€ deploy.sh               # Deployment script
â”œâ”€â”€ tests/                      # Test suite (74 tests)
â”‚   â”œâ”€â”€ conftest.py             # Pytest fixtures
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â”œâ”€â”€ test_etl_service.py
â”‚   â”œâ”€â”€ test_etl_recovery.py
â”‚   â”œâ”€â”€ test_schema_drift.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/
â”‚   â””â”€â”€ crypto_data.csv         # Sample CSV data
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml                  # CI/CD pipeline
â”œâ”€â”€ docker-compose.yml          # Local development
â”œâ”€â”€ Dockerfile                  # Multi-stage build
â”œâ”€â”€ Makefile                    # Common commands
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Docker & Docker Compose
- PostgreSQL (or use Docker)

### Setup

1. Clone the repository:
```bash
git clone https://github.com/Rajveerjagtap/kasparro-crypto-etl.git
cd kasparro-crypto-etl
```

2. Copy environment file:
```bash
cp .env.example .env
```

3. Start with Docker:
```bash
make docker-up
```

Or run locally:
```bash
make install
make db-up
make run
```

API will be available at `http://localhost:8000`

---

## ğŸ“¡ API Reference

### Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | System health check (DB + ETL status) |
| `/api/v1/data` | GET | List crypto data with filters |
| `/api/v1/stats` | GET | Aggregated statistics |
| `/api/v1/etl/run/{source}` | POST | Trigger ETL for a source |
| `/api/v1/etl/jobs` | GET | View ETL job history |
| `/api/v1/runs/compare` | GET | Compare two ETL runs |
| `/api/v1/metrics` | GET | Prometheus metrics |

### Query Parameters for `/api/v1/data`

| Parameter | Type | Description |
|-----------|------|-------------|
| `symbol` | string | Filter by symbol (e.g., "BTC") |
| `source` | string | Filter by source (coingecko, coinpaprika, csv) |
| `limit` | int | Number of records (default: 100) |
| `offset` | int | Pagination offset (default: 0) |

### Example Requests

```bash
# Health check
curl http://localhost:8000/health

# Get BTC data from CoinGecko
curl "http://localhost:8000/api/v1/data?symbol=BTC&source=coingecko"

# Trigger CSV ETL (synchronous)
curl -X POST "http://localhost:8000/api/v1/etl/run/csv?sync=true"

# Get statistics
curl http://localhost:8000/api/v1/stats
```

---

## ğŸ—„ Database Schema

### Tables

| Table | Purpose |
|-------|---------|
| `raw_data` | Stores original JSON payloads for auditability |
| `unified_crypto_data` | Normalized cryptocurrency data (canonical schema) |
| `etl_jobs` | ETL job tracking with status and metrics |

### Unified Crypto Data Schema

```sql
CREATE TABLE unified_crypto_data (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(20) NOT NULL,
    price_usd NUMERIC(20, 8),
    market_cap NUMERIC(30, 2),
    volume_24h NUMERIC(30, 2),
    source data_source_enum NOT NULL,
    timestamp TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE (symbol, timestamp)
);
```

---

## â° Scheduled ETL

The scheduler service runs ETL jobs automatically:

```bash
# Default: every hour (3600 seconds)
SCHEDULE_INTERVAL=3600

# Run scheduler locally
python -m app.scheduler

# Or with Docker Compose (included in `make up`)
docker compose up scheduler
```

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection string | Required |
| `SCHEDULE_INTERVAL` | ETL interval in seconds | `3600` |
| `LOG_LEVEL` | Logging level | `INFO` |
| `COINGECKO_KEY` | CoinGecko API key (optional) | `None` |
| `COINPAPRIKA_KEY` | CoinPaprika API key (optional) | `None` |

---

## ğŸ§ª Testing

```bash
# Run all tests (74 tests)
make test

# Run with coverage
pytest tests/ -v --cov=app

# Run specific test file
pytest tests/test_extractors.py -v
```

### Test Categories
- **Unit Tests**: Extractors, models, config, schemas
- **Integration Tests**: API endpoints, database operations
- **Recovery Tests**: ETL failure injection and retry logic
- **Schema Drift Tests**: Forward compatibility with API changes

---

## â˜ï¸ Cloud Deployment (Azure)

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Azure Container Apps                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Backend API   â”‚          â”‚    Scheduler    â”‚          â”‚
â”‚  â”‚    (FastAPI)    â”‚          â”‚  (APScheduler)  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                             â”‚                    â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                       â–¼                                      â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚           â”‚     PostgreSQL      â”‚                           â”‚
â”‚           â”‚ (Flexible Server)   â”‚                           â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CI/CD Pipeline

The GitHub Actions workflow (`.github/workflows/ci.yml`) includes:

1. **Lint & Type Check**: Ruff + MyPy
2. **Unit Tests**: Pytest with PostgreSQL service container
3. **Build Docker Image**: Multi-stage build
4. **Integration Tests**: Docker Compose end-to-end tests
5. **Publish**: Push to GitHub Container Registry
6. **Deploy**: Azure Container Apps via Bicep

### Manual Deployment

```bash
# Deploy to Azure
cd azure
./deploy.sh
```

---

## ğŸ“Š Monitoring

### Health Check Response

```json
{
  "status": "healthy",
  "database": {
    "connected": true,
    "latency_ms": 5.2
  },
  "etl": {
    "last_job_status": "success",
    "last_run": "2024-01-15T12:00:00Z",
    "record_count": 150
  }
}
```

### Prometheus Metrics

```
# ETL job counts by source and status
kasparro_etl_jobs_total{source="coingecko",status="success"} 24

# Records processed
kasparro_etl_records_processed_total{source="coingecko"} 1200

# HTTP request latency
kasparro_http_request_duration_seconds_bucket{endpoint="/api/v1/data",le="0.5"} 100
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Rajveer Jagtap**
- GitHub: [@Rajveerjagtap](https://github.com/Rajveerjagtap)
